# -*- coding: utf-8 -*-
"""LM_Conversacional_basico.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JGc21qEz7m5DPluCs57l4R1gMkpk0ghS

## Paso 1: Instalación de librerías necesarias
"""

!pip install transformers datasets

"""## Paso 2: Importaciones"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments
from datasets import load_dataset
import torch

"""## Paso 3: Cargar modelo y tokenizer"""

model_name = "distilgpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained(model_name)

"""## Paso 4: Cargar el dataset conversacional"""

dataset = load_dataset("daily_dialog")
texts = [dialog for dialog in dataset['train']['dialog'][:1000]]

"""## Paso 5: Tokenización y conversión"""

def tokenize_and_format(example):
    tokens = tokenizer(" ".join(example), truncation=True, padding="max_length", max_length=128)
    tokens["labels"] = tokens["input_ids"].copy()  # <- esto es lo que faltaba
    return tokens

tokenized_dataset = dataset["train"].select(range(1000)).map(tokenize_and_format, batched=False)
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

"""## Paso 6: Definir los parámetros del entrenamiento"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=2,
    num_train_epochs=1,
    save_steps=500,
    logging_steps=100,
    report_to="none"  # <- Esto desactiva WandB correctamente
)

"""## Paso 7: Entrenamiento del modelo"""

import os
os.environ["WANDB_DISABLED"] = "true"
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

trainer.train()

"""## Paso 8: Guardar el modelo"""

model.save_pretrained("./trained_model")
tokenizer.save_pretrained("./trained_model")

!zip -r trained_model.zip trained_model/
from google.colab import files
files.download("trained_model.zip")